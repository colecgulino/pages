# Neural Network Library in Numpy

To improve on my deep learning fundamentals, I wanted to understand how backpropogation worked for most of the fundamental building blocks of neural networks (i.e. convolution, attention, etc.) Here I will document the results for others who are studying as well.

[github](https://github.com/colecgulino/numpy-nn)

[layer.py](layer_py.md)

&rarr; [activations.py](activations_py.md)

&rarr; [attention.py](attention_py.md)

&rarr; [convolution.py](convolution_py.md)

&rarr; [dense.py](dense_py.md)

&rarr; [initializers.py](initializers_py.md)

&rarr; [losses.py](losses_py.md)

&rarr; [normalization.py](normalization_py.md)

&rarr; [optimizers.py](optimizers_py.md)

&rarr; [regularization.py](regularization_py.md)

&rarr; [transformer.py](transformer_py.md)
