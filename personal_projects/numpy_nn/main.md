# Neural Network Library in Numpy

To improve on my deep learning fundamentals, I wanted to understand how backpropogation worked for most of the fundamental building blocks of neural networks (i.e. convolution, attention, etc.) Here I will document the results for others who are studying as well.

[github](https://github.com/colecgulino/numpy-nn)

# Neural Network Layers

[layer.py](layer_py.md)

&rarr; [dense.py](dense_py.md)

&rarr; [convolution.py](convolution_py.md)

&rarr; [transformer.py](transformer_py.md)

&rarr; [attention.py](attention_py.md)

[activations.py](activations_py.md)

[initializers.py](initializers_py.md)

[normalization.py](normalization_py.md)

# Optimization
[losses.py](losses_py.md)

[optimizers.py](optimizers_py.md)

[regularization.py](regularization_py.md)